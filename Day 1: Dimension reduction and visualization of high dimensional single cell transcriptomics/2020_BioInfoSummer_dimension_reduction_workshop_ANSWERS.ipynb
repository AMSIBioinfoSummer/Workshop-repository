{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\newcommand{\\bu}{\\mathbf{u}}\n",
    "\\newcommand{\\bv}{\\mathbf{v}}\n",
    "\\newcommand{\\bw}{\\mathbf{w}}\n",
    "\\newcommand{\\bx}{\\mathbf{x}}\n",
    "\\newcommand{\\by}{\\mathbf{y}}\n",
    "\\newcommand{\\bz}{\\mathbf{z}}\n",
    "\\newcommand{\\bC}{\\mathbf{C}}\n",
    "\\newcommand{\\bD}{\\mathbf{D}}\n",
    "\\newcommand{\\bI}{\\mathbf{I}}\n",
    "\\newcommand{\\bU}{\\mathbf{U}}\n",
    "\\newcommand{\\bV}{\\mathbf{V}}\n",
    "\\newcommand{\\bW}{\\mathbf{W}}\n",
    "\\newcommand{\\bX}{\\mathbf{X}}\n",
    "\\newcommand{\\bY}{\\mathbf{Y}}\n",
    "\\newcommand{\\bSigma}{\\mathbf{\\Sigma}}\n",
    "\\newcommand{\\bLambda}{\\mathbf{\\Lambda}}\n",
    "\\newcommand{\\bbN}{\\mathbb{N}}\n",
    "\\newcommand{\\bbR}{\\mathbb{R}}\n",
    "\\newcommand{\\cU}{\\mathcal{U}}\n",
    "\\newcommand{\\rNN}{\\mathrm{NN}}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Dimension reduction and clustering libraries\n",
    "import umap\n",
    "import hdbscan\n",
    "\n",
    "import chart_studio as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "\n",
    "axis_layout = dict(autorange=True, showgrid=False, zeroline=False, showline=False,\n",
    "                   ticks='', showticklabels=False, title='', showbackground=False)\n",
    "layout = dict(width=400, height=400, showlegend=False,\n",
    "              margin=go.layout.Margin(l=0, r=0, b=0, t=0, pad=10),\n",
    "              paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)',\n",
    "              scene=dict(xaxis = axis_layout, yaxis=axis_layout, zaxis=axis_layout),\n",
    "              scene_aspectmode='cube', scene_aspectratio=dict(x=1, y=1, z=1))\n",
    "\n",
    "def cm_2_rgbstr(c):\n",
    "    return f'rgb({int(round(c[0]*255))}, {int(round(c[1]*255))}, {int(round(c[2]*255))})'\n",
    "def cma_2_rgbstr(c, a):\n",
    "    return f'rgba({int(round(c[0]*255))}, {int(round(c[1]*255))}, {int(round(c[2]*255))}, {a})'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1: The curse of high dimensional data\n",
    "\n",
    "Lets say we have $n$ data points, each containing $\\large d$ attributes. Each data point we label $\\large \\bx_i$ with $\\large i=1,\\ldots,n$.\n",
    "\n",
    "Each field could be an integer $(1,2,\\ldots)$, or a real number $(\\mathbb{R})$, or even a label. For example, in single cell data we often have a vector of _reads_, which is a list of the number of times we have read a fragment, so a data point could look like\n",
    "\n",
    "$$\\large \n",
    "\\bx_i = (12, 0, 39, 40, 1258, 0, 8, 39, \\ldots, 27)\n",
    "$$\n",
    "\n",
    "This list has $d$ numbers in it.\n",
    "\n",
    "In this talk, we assume each data point is a vector (list) of _real numbers_ of length $d$, that is \n",
    "\n",
    "$$ \\large\n",
    "\\bx_i \\in \\bbR^d\n",
    "$$\n",
    "\n",
    "Yes, single cell datasets don't often look like this, and we are ignoring positivity and problems like _zero inflation_ that we typically see, but for our purposes it is simpler to consider Euclidian space.\n",
    "\n",
    "We are interested in the fact that $\\large d$ is very large (_typically in the thousands!_), meaning that our data is _high dimensional_. Such high dimensional data is difficult to process for (mathematically) very fundamental reasons.\n",
    "\n",
    "For example, _clustering_ typically doesn't work directly on high dimensional data.\n",
    "\n",
    "Throughout the rest of this workbook, we will write $\\large \\bX$ to denote the _data matrix_, that is the matrix of size $\\large(n \\times d)$ in which each data point $\\bx_i$ (which is a vector of length $d$) is the $i$-th row of $\\bX$, that is $\\large[\\bx_i]_j = \\bX_{i,j}$.\n",
    "\n",
    "### Consider $n$ uniformly distributed points in the unit cube $[0,1]^d$\n",
    "\n",
    "\n",
    "Lets generate a data matrix where each component is independently and uniformly distributed in $\\large[0,1]$, effectively meaning that each row of $\\bX$ is a vector that is independently and uniformly distributed in $\\large [0,1]^d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "d = 10\n",
    "\n",
    "# If we consider each column to be a data vector (i.e. x_i = X[:, i], then this\n",
    "# generates n random points that are uniformly distributed in [0,1]^d\n",
    "X = np.random.random((n,d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. In high dimensions all distances become the same\n",
    "\n",
    "On most reasonable data sets $\\large \\{ \\bx_i \\}_{i=1}^n$, distances between all pairs of data points become very similar: \n",
    "\n",
    "$$\n",
    "\\large \\max_{i \\neq j} \\| \\bx_i - \\bx_j \\| \\approx \\min_{i \\neq j} \\| \\bx_i - \\bx_j \\| \\quad\n",
    "\\text{ as $d$ gets larger.}\n",
    "$$\n",
    "\n",
    "*(We use the notation $\\| \\bx \\|$ to denote the length of the vector $\\bx$)*\n",
    "\n",
    "We can demonstrate this phenomenon with the uniformly distributed data we just generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distances between each pair of points in the data set\n",
    "dists = np.zeros((n,n))\n",
    "for i in range(n):\n",
    "    for j in range(i,n):\n",
    "        dists[i,j] = dists[j,i] = np.linalg.norm(X[i] - X[j]) # EXERCISE: FILL IN HERE\n",
    "\n",
    "dists_max = dists.max()\n",
    "dists_min = dists[np.triu_indices(n, k=1)].min()\n",
    "\n",
    "print(f'{d} dimensions, {n} random points in [0,1]^{d}:')\n",
    "print(f'max dist = {dists_max}, min dist = {dists_min}')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. In high dimensions all of the data lies on the edges\n",
    "\n",
    "Consider the unit hypercube $\\large [0,1]^d$. The mean of the uniform distribution $\\large \\cU([0,1]^d)$ on it is the centre point $\\large (1/2,\\ldots,1/2)$. But almost all the data will be nowhere near the mean! Why?\n",
    "\n",
    "Lets consider the box $\\large B = [1/4, 3/4]^d$. It surrounds the mean / centre point, it sits within $[0,1]^d$, and each side has length $1/2$. But what is the volume of this box?\n",
    "\n",
    "$$\n",
    "\\large | B | = (1/2)^d\n",
    "$$\n",
    "\n",
    "which becomes _very small for large_ $d$! For example at $d = 10$ we have $\\large | B | = (1/2)^{10} \\approx 1/1000$. This means only _one in a thousand_ of our randomly generated points are likely to be in the box around the mean!\n",
    "\n",
    "Even if we increased the sides of $B$ to $0.999$, eventually we get that $|B| \\to 0$ for large enough $d$. So, most of the probability lies at the edges of the unit cube $[0,1]^d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "d = 10\n",
    "X = np.random.random((n,d))\n",
    "\n",
    "# Lets count the number of points in X that are \"near the mean\", by which we mean within the box [1/4, 3/4]^d\n",
    "num_x_in_B = 0\n",
    "\n",
    "for x in X:\n",
    "    # This returns a vector of booleans\n",
    "    is_x_in_B = (x > 1/4) & (x < 3/4) # EXERCISE: FILL IN HERE\n",
    "\n",
    "    #print(x)\n",
    "    #print(is_x_in_B)\n",
    "    #print(np.all(is_x_in_B))\n",
    "    \n",
    "    if np.all(is_x_in_B):\n",
    "        # Increment the count if ALL coordinates of this point within [1/4, 3/4]?\n",
    "        num_x_in_B += 1\n",
    "        \n",
    "print(f'In dimension {d} there are {num_x_in_B} points out of {n} that are in B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This means everyone is an outlier in high dimensional statistics!\n",
    "\n",
    "Lets consider a real world example, the _diabetes_ data set provided as part of ```sklearn```. It is a 10 dimensional data set, i.e. each data point has _10 attributes_.\n",
    "\n",
    " 1. age in years\n",
    " 2. sex\n",
    " 3. bmi body mass index\n",
    " 4. bp average blood pressure\n",
    " 5. s1 tc, T-Cells (a type of white blood cells)\n",
    " 6. s2 ldl, low-density lipoproteins\n",
    " 7. s3 hdl, high-density lipoproteins\n",
    " 8. s4 tch, thyroid stimulating hormone\n",
    " 9. s5 ltg, lamotrigine\n",
    " 10. s6 glu, blood sugar level\n",
    " \n",
    " Each field is recentered and scaled to be in the range $[-0.2, 0.2]$ with mean $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "dia_data = sklearn.datasets.load_diabetes().data\n",
    "n = dia_data.shape[0] # Number of samples\n",
    "d = dia_data.shape[1] # Number of attributes (dimensionality)\n",
    "\n",
    "print(f'Data shape: {n} by {d}')\n",
    "print(f'Mean of each field: \\n {dia_data.mean(axis=0)}')\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 4))\n",
    "# Lets plot BMI vs blood pressure\n",
    "axs[0].scatter(dia_data[:,2], dia_data[:,3])\n",
    "axs[0].set_xlabel('Body Mass Index')\n",
    "axs[0].set_ylabel('Avg blood pressure')\n",
    "\n",
    "# Lets plot age vs BMI\n",
    "axs[1].scatter(dia_data[:,0], dia_data[:,2])\n",
    "axs[1].set_xlabel('Age')\n",
    "axs[1].set_ylabel('Body Mass Index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now lets find all the points that are within the restricted range of -0.05 to 0.05\n",
    "selection = (dia_data > -0.05) & (dia_data < 0.05) # EXERCISE: FILL IN HERE\n",
    "\n",
    "for i in range(d):\n",
    "    \n",
    "    num_selected = selection[:, i].sum()\n",
    "    print(f'Number of samples with attribute {i} within [-0.05, 0.05]: {num_selected}')\n",
    "\n",
    "    total_selection = np.all(selection[:,:i+1], axis=1)\n",
    "    total_num_selected = total_selection.sum()\n",
    "    print(f'Number of samples with all attributes up to {i} within [-0.05, 0.05]: {total_num_selected} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $k$-means fails in high dimension\n",
    "\n",
    "Why? Lets inspect the algorithm. Remember, with $k$-means we specify the number of clusters $k$ as a parameter. \n",
    "\n",
    "1. Generate $k$ starting centroid points in $\\mathbb{R}^d$, which we label $c_j$ for $j=1,\\ldots,k$\n",
    "2. For each data point $x_i$, find the centroid $c_j$ that is closest to it, and we now say $x_i$ is in cluster $j$\n",
    "3. For each cluster $j$, we set the new centroid $c_j$ to be the average point of all $x_i$ in cluster $j$\n",
    "4. Repeat from step 2 until the centroids $c_j$ stop moving.\n",
    "\n",
    "The problem? In high dimensions, the distances $\\| x_i - c_j \\|$ are almost all the same (as we saw above), so the $x_i$ get attributed to arbitrary clusters, and the centroids $c_j$ do not converge to anything meaningful.\n",
    "\n",
    "### Lets generate a really simple example\n",
    "\n",
    "Lets consider $n$ points where the first attribute (or coordinate) is set to 0 for half the points, and 1 for the other half, and the rest of the attributes are independent and uniformly distributed in $[0,1]$\n",
    "\n",
    "Our data matrix will look like\n",
    "\n",
    "$\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "0 & x_{1,2} & \\cdots & x_{1,d} \\\\\n",
    "0 & x_{2,2} & \\cdots & x_{2,d} \\\\\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "0 & x_{n/2-1, 2} & \\cdots & x_{n/2-1,d} \\\\\n",
    "1 & x_{n/2, 2} & \\cdots & x_{n/2,d} \\\\\n",
    "1 & x_{n/2+1, 2} & \\cdots & x_{n/2+1,d} \\\\\n",
    "\\vdots & \\vdots &  & \\vdots \\\\\n",
    "1 & x_{n, 2} & \\cdots & x_{n,d} \\\\\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 10\n",
    "n = 20\n",
    "\n",
    "np.random.seed(0)\n",
    "X = np.random.random((n, d))\n",
    "\n",
    "# We set the first attribute to 0 for half the samples and 1 for the other half\n",
    "X[:int(n/2), 0] = 0 # EXERCISE: FILL IN HERE\n",
    "X[int(n/2):, 0] = 1 # EXERCISE: FILL IN HERE\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=2).fit(X) # EXERCISE: FILL IN HERE\n",
    "\n",
    "print('What the labels \"should\" be: ')\n",
    "print(X[:, 0].astype(np.int))\n",
    "print('\\nWhat k-means infers: ')\n",
    "print(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So is there anything that can save us from this predicament?\n",
    "\n",
    "### It could be that the data lives on some hidden structure or _manifold_\n",
    "\n",
    "The data may actually be lying on some curved manifold embedded in the high dimensional space. Some natural questions arise:\n",
    "\n",
    " - Can we detect this underlying structure using the data somehow?\n",
    " - Can we then map the data to a lower dimensional space, but still retain the structure of the data?\n",
    "\n",
    "This idea of mapping to a lower dimensional space is called _dimension reduction_. We want to reduce the dimensionality of data so that we can apply our statistical learning algorithms succesfully.\n",
    "\n",
    "### An example of data that lives on an underlying _manifold_:\n",
    "\n",
    "Lets consider an artificial example again. Lets say we have some data points that are distributed independently and uniformly on the unit square $[0,1]^2$. Now, lets say our observed data are points in 3 dimensions where the data points are the vectors $[x, y, f(x)]$, for some smooth function $f$. Consider the example in the following code. \n",
    "\n",
    "Rotate the 3d plot to see what's going on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 3\n",
    "n = 500\n",
    "\n",
    "np.random.seed(0)\n",
    "X = np.random.random((n, d))\n",
    "X[:, 2] = np.sin(4 * np.pi * X[:, 0])\n",
    "\n",
    "import matplotlib.cm\n",
    "jet_colors = [ matplotlib.cm.jet(x) for x in X[:,0] ]\n",
    "c_marker = dict(size=2.5, symbol='circle', color=jet_colors)\n",
    "\n",
    "fig_data = [go.Scatter3d(x=X[:,0], y=X[:,1], z=X[:,2], mode='markers', marker=c_marker, showlegend=False)]\n",
    "\n",
    "fig = go.Figure(data=fig_data, layout=layout)\n",
    "camera = dict(eye=dict(x=2., y=0, z=0.))\n",
    "fig.update_layout(scene_camera=camera)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension reduction through principal component analysis (PCA)\n",
    "\n",
    "PCA is often used for dimension reduction. We perform an orthogonal projection on to the space of truncated principle components or _eigen-vectors_. It is a _linear_ method though, which can have its drawbacks.\n",
    "\n",
    "How is it calculated?\n",
    "\n",
    "Given our data points $\\{x_i\\}_{i=1}^n \\subset \\mathbb{R}^d$ or data matrix $\\bX$, we take the mean $\\mu$ and covariance matrix $\\bC$ given by\n",
    "\n",
    "$$\\large\n",
    "\\begin{align} \n",
    "\\mu &= \\frac {1}{n} \\sum_{i=1}^n x_i \\\\\n",
    "%\\bC_{i,j} &= \\frac{1}{n-1} (x_i - \\mu) \\otimes ( x_j - \\mu)\n",
    "\\bC &= \\frac{1}{n-1} (\\bX - \\mu)^T (\\bX - \\mu)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $(\\bX - \\mu)$ is the matrix $\\bX$ of size $n \\times d$ with the vector $\\mu$ of size $d$ subtracted from every row. Thus $\\bC$ is a matrix of size $d \\times d$.\n",
    "\n",
    "The _principal components_ are given by the eigenvalues and eigenvectors of $\\bC$, that is $\\large \\bC = \\bV \\bSigma \\bV^T$, where $\\bSigma$ is just the eigenvalues along the diagonal, and $\\bV$ is an orthonormal matrix, the columns of which are the eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of the points and calculate the mean-free point set\n",
    "mean = X.mean(axis=0) # EXERCISE: FILL IN HERE\n",
    "X_mf = X - mean # EXERCISE: FILL IN HERE\n",
    "\n",
    "# The covariance is calculated form the mean free points\n",
    "Cov = (X_mf.T @ X_mf) / (n-1) # EXERCISE: FILL IN HERE\n",
    "\n",
    "# np.linalg.eigh calculates the eigen-decomposition for a Hermitian (i.e. symmetric) matrix\n",
    "Sigma, V = np.linalg.eigh(Cov)\n",
    "\n",
    "# Now we plot the PCA \n",
    "fig_pca = []\n",
    "for sigma, v in zip(Sigma, V.T):\n",
    "    fig_pca.append(go.Scatter3d(x=[mean[0], mean[0]+np.sqrt(sigma)*v[0]], \n",
    "                                y=[mean[1], mean[1]+np.sqrt(sigma)*v[1]],\n",
    "                                z=[mean[2], mean[2]+np.sqrt(sigma)*v[2]],\n",
    "                                mode='lines', line=dict(width=4, color='rgba(0,0,0,0.8)')))\n",
    "    fig_pca.append(go.Scatter3d(x=[mean[0], mean[0]+v[0]], \n",
    "                                y=[mean[1], mean[1]+v[1]],\n",
    "                                z=[mean[2], mean[2]+v[2]],\n",
    "                                mode='lines', line=dict(width=1, dash='dash', color='rgba(0,0,0,0.8)')))\n",
    "\n",
    "fig = go.Figure(data=fig_data+fig_pca, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we project the data to the two largest principle componants, i.e. the two eigenvectors with largest eigenvalues\n",
    "\n",
    "We know that $\\large \\bV$ is an orthonormal matrix, so for any data vector $\\bx$, $\\bV^T (\\bx - \\mu)$ will give us the data in the coordinates of the principal components.\n",
    "\n",
    "If we take coordinates of the two biggest principle coordinates and multiply by the corresponding columns of $\\bV$, we effectively get the data vector projected to the linear subspace of the two biggest principal components\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we translate all our data coordinates to PCA coordinates\n",
    "X_pca = V.T @ X_mf.T\n",
    "\n",
    "# We use the two largest principle components (which are the last two from the eigen-decomposition)\n",
    "# This effectively calculates the projection.\n",
    "X_proj = (np.outer(X_pca[1], V[:,1]) + np.outer(X_pca[2], V[:,2]) + mean).T\n",
    "\n",
    "c_marker = dict(size=2.5, symbol='circle', color=jet_colors)\n",
    "fig_proj = [go.Scatter3d(x=X_proj[0,:], y=X_proj[1,:], z=X_proj[2,:], mode='markers', marker=c_marker, showlegend=False)]\n",
    "\n",
    "fig = go.Figure(data=fig_proj+fig_pca, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what's the problem with applying PCA in this case? We can see from the colour of the points that dimension reduction through PCA has just _squashed_ the manifold in some direction.\n",
    "\n",
    "This squashing is not the result we're looking for. For example, the red points and the light-blue points are all mixed up, but we can see from the original plot that we would ideally want to map the underlying sinusoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2: Dimension reduction through _manifold learning_\n",
    "\n",
    "We want something that can sense the _underlying manifold_ that the points sit upon. In a sense, knowing how the points are connected to each other is equivalent information.\n",
    "\n",
    "So, we want to gain some sense of what the nearest neigbours _should_ be to each point, and which points should _not_ be connected as nearest neighbours.\n",
    "\n",
    "This is not an easy task! \n",
    "\n",
    "### Lets see what happens if we naively calculate the nearest neighbours in this data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "k_nn = 4\n",
    "nbrs = NearestNeighbors(n_neighbors=k_nn, algorithm='ball_tree').fit(X)\n",
    "distances, indices = nbrs.kneighbors(X)\n",
    "print(indices)\n",
    "\n",
    "fig_nn = []\n",
    "for i in range(n): \n",
    "    # Loop over each point in the data set\n",
    "    for j in range(1,k_nn): \n",
    "        # Loop over each nearest neighbour in the data set\n",
    "        # Draw a thin black line between each detected nearest neighbour\n",
    "        fig_nn.append(go.Scatter3d(x=[X[indices[i,0]][0], X[indices[i,j]][0]], \n",
    "                                   y=[X[indices[i,0]][1], X[indices[i,j]][1]],\n",
    "                                   z=[X[indices[i,0]][2], X[indices[i,j]][2]],\n",
    "                                   mode='lines', line=dict(width=1, color='rgba(0,0,0,0.5)')))\n",
    "        \n",
    "fig = go.Figure(data=fig_data+fig_nn, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We see lots of connections that are \"false\" in the sense that they are connections that are not really nearest neighbours if we were a small ant that had to walk only on the underlying manifold.\n",
    "\n",
    "Bonus extra exercise: Could you only detect \"correct\" nearest neighbours by looking at the $(x,y)$ coordinates only? Copy the cell above and modify below to try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the $k$-nearest neighbours is the first step to most popular nonlinear dimension reduction algorithms, including:\n",
    " - ### ISOMAP (Tenenbaum & Langford, 2000)\n",
    " - ### $t$-SNE ($t$ stochastic neighour embedding, Hinton & Roweis, 2002)\n",
    " - ### UMAP (uniform manifold approximation and projection, McInnes, Healy & Melville, 2018)\n",
    "\n",
    "Today we are going to look further in to __UMAP__, which is fast becoming the most popular choice for dimension reduction. How does it work? Roughly it follows the following steps:\n",
    "\n",
    "# __An overview of the steps in the UMAP algorithm__\n",
    "\n",
    "1. __Compute the ```n_neighbors``` number of nearest neighbours (```n_neighbors``` is a parameter to the code)__\n",
    "2. __Calculate a local sense of \"distance scaling\" which we label $\\large \\sigma_i$, at each point $\\large \\bx_i$. This distance is based on how far away the nearest neighbours to $\\bx_i$ are.__\n",
    "3. __A \"connection weight\" between $\\bx_i$ and $\\bx_j$ is calculated based on $\\sigma_i$ and $\\sigma_j$.__\n",
    "4. __Create a set of points in a space of dimension ```n_components``` (this is also a parameter given to the code), and adjust their locations until their pairwise distances roughly match those of the distances calculated in step 3. This is a large optimisation problem.__\n",
    "\n",
    "Lets look a little closer at each step:\n",
    "\n",
    "## Step 1. Finding the nearest neighbours\n",
    "\n",
    "In practice this can be computed in a straightforward manner, but UMAP uses a particular accelerated algorithm. In the cell below we pick two nearest neighbours and plot their location. To make things simple, we chose $\\large \\bx_0$ as a point to look at (that is, we chose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_sel = 0\n",
    "sub_choice = indices[i_sel]\n",
    "print(f'The indices of the nearest neighbours are: {sub_choice} \\n')\n",
    "\n",
    "# Sub choice of data points: this \n",
    "X_sub = X[sub_choice,:]\n",
    "print(f'The coordinates of x_{i_sel} and its nearest neighbours are:\\n {X_sub}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot this little selection of points and their nearest neighbour connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make a see-through scatter plot\n",
    "fig_bg_data = [go.Scatter3d(x=X[:,0], y=X[:,1], z=X[:,2], mode='markers', opacity=0.3, marker=c_marker, showlegend=False)]\n",
    "\n",
    "# We plot our litte sub choice of \n",
    "jet_sub_colors = [ matplotlib.cm.jet(x) for x in X_sub[:,0] ]\n",
    "c_sub_marker = dict(size=4, symbol='circle', color=jet_sub_colors, line=dict(color='Black', width=2))\n",
    "fig_sub_data = [go.Scatter3d(x=X_sub[:,0], y=X_sub[:,1], z=X_sub[:,2], mode='markers', marker=c_sub_marker, showlegend=False)]\n",
    "\n",
    "# Plot the connections between to the nearest neighbours of X[i_sel] and label them\n",
    "fig_nn_sub = []\n",
    "for j in sub_choice[1:]: \n",
    "    # Loop over each nearest neighbour in the data set\n",
    "    # Draw a thin black line between each detected nearest neighbour\n",
    "    fig_nn_sub.append(go.Scatter3d(x=[X[i_sel][0], X[j][0]], \n",
    "                               y=[X[i_sel][1], X[j][1]],\n",
    "                               z=[X[i_sel][2], X[j][2]],\n",
    "                               mode='lines', line=dict(width=1, color='rgba(0,0,0,0.5)')))\n",
    "fig_nn_sub.append(go.Scatter3d(x=X[sub_choice][:,0], \n",
    "                               y=X[sub_choice][:,1],\n",
    "                               z=X[sub_choice][:,2],\n",
    "                               mode='text', text=[rf'x_{j}' for j in sub_choice]))\n",
    "\n",
    "fig = go.Figure(data=fig_sub_data + fig_bg_data + fig_nn_sub, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Calculating the distance scaling $\\sigma_i$\n",
    "\n",
    "The nearest neighbour set $\\large \\rNN_k(i)$ is the set of indices of points that are the $k$ closest points to $\\bx_i$. Lets also consider the distance to the single _nearest neighbour_ to $\\bx_i$, and denote it as\n",
    "\n",
    "$$\\large r_i = \\min_{j \\neq i} \\| x_i - x_j \\|$$\n",
    "\n",
    "### Now, we set $\\sigma_i$ such that:\n",
    "\n",
    "$$\n",
    "\\large\n",
    "\\sigma_i\\text{-scaled weights sum } = \\sum_{j \\in \\rNN_k(i)}  \\exp\\left( \\frac{-(\\| \\bx_i - \\bx_j \\| - r_i)}{\\sigma_i}\\right) = \\log_2(k)\n",
    "$$\n",
    "\n",
    "The justification of this equation would be outside of the scope of this tutorial.\n",
    "\n",
    "Essentially this equation takes care of two important problems. \n",
    " 1. The use of $\\large r_i$ as a base-case distance takes care of the fact that high dimensional data is all \"spread out\" as we saw earlier. \n",
    " 2. Outliers are \"de-prioritised\" by the exponential. Any nearest neighbours that are in fact far away, compared to the other nearest neighbours, will contribute very little to the calculation of $\\large \\sigma_i$.\n",
    "\n",
    "Lets code up this calculation and inspect what happens for hand-picked values of $\\large \\sigma_i$. \n",
    "\n",
    "**See if you can find the value of $\\large \\sigma_i$ that gives the right value of the sum of weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_i = 0.01\n",
    "\n",
    "i_sel = 0\n",
    "sub_choice = indices[i_sel]\n",
    "print(f'Nearest neighbours: {sub_choice} \\n')\n",
    "\n",
    "# Calculate r_i\n",
    "r_i = np.inf\n",
    "for j in sub_choice[1:]:\n",
    "    r_i = min(r_i, np.linalg.norm(X[i_sel] - X[j])) # EXERCISE: FILL IN HERE\n",
    "\n",
    "# Calculate the sigma_i scaled weights sum\n",
    "normed_ws = 0.\n",
    "for j in sub_choice[1:]:\n",
    "    normed_ws += np.exp( -(np.linalg.norm(X[i_sel] - X[j]) - r_i) / sigma_i)\n",
    "\n",
    "print(f'sigma_i = {sigma_i}')\n",
    "print(f'sigma_i scaled weights sum = {normed_ws}')\n",
    "print(f'k = {k_nn}, Log_2(k) = {np.log2(k_nn)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Determining the connection weight\n",
    "\n",
    "Now we have the single-direction connection weight\n",
    "\n",
    "$$ \\large \n",
    "w_{j | i} = \n",
    "\\begin{cases}\n",
    "\\exp \\left( \\frac{-(\\| \\bx_i - \\bx_j \\| - r_i)}{\\sigma_i} \\right) & \\text{if } j \\in \\rNN_k(i) \\\\\n",
    "0 & \\text{otherwise.} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Note that we don't necessarily have that $\\large w_{j | i}$ is equal to $\\large w_{i | j}$, so we symmetrise this notion,\n",
    "defining the _connection weight_ between $\\bx_i$ and $\\bx_j$ as\n",
    "\n",
    "$$\\large\n",
    "w_{ij} = w_{j | i} + w_{i | j} - w_{j | i} w_{i | j}.\n",
    "$$\n",
    "\n",
    "## Step 4: Determining the lower dimensional embedding\n",
    "\n",
    "One of the most important hyper-parameters for UMAP is the _target dimension_, labelled ```n_components``` in the code. This is the number of dimensions that we wish to embed the data in.\n",
    "\n",
    "The final step creates a set of points of dimension ```n_components```, that is $\\{ \\by_i \\}_{i=1}^n \\subset \\bbR^{\\mathrm{n\\_components}}$. The points $\\by_i$ are all moved around the space until their weights $w_{ij}$ are as close as possible to those weights calculated in Step 3.\n",
    "\n",
    "And that's it! That is roughly the magic of UMAP.\n",
    "\n",
    "### Lets try using UMAP on our sinusoidal manifold:\n",
    "\n",
    "UMAP can be invoked by calling ```umap.UMAP(n_components = (??), n_neighbors = (??)).fit_transform(X)```. The interface was chosen to be the same as ```scikit-learn```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='white', rc={'figure.figsize':(10,8)})\n",
    "\n",
    "sin_embedding = umap.UMAP(n_components=2, n_neighbors=30).fit_transform(X)\n",
    "plt.scatter(sin_embedding[:, 0], sin_embedding[:, 1], c=jet_colors, s=4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 3: Using UMAP on high dimensional data for clustering\n",
    "## UMAP for clustering MNIST data\n",
    "\n",
    "Each data point is an image of 28 by 28 pixels, each pixel has an integer value between 0 and 255.\n",
    "\n",
    "$28 \\times 28 = 784$, so the dimensionality of the data is 784."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "mnist = sklearn.datasets.fetch_openml('mnist_784')\n",
    "mnist.target = mnist.target.astype(int)\n",
    "\n",
    "print(f'MNIST data has {mnist.data.shape[0]} data points of dimensionality {mnist.data.shape[1]}')\n",
    "\n",
    "fig, axs = plt.subplots(1,4, figsize=(14, 4))\n",
    "for i, digit in enumerate(mnist.data[:4]):\n",
    "    fig.sca(axs[i])\n",
    "    sns.heatmap(digit.reshape((28,28)), cbar=False, cmap=\"Greys\")\n",
    "    axs[i].axes.get_xaxis().set_visible(False)\n",
    "    axs[i].axes.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we create an embedding using UMAP to plot the clusters of data\n",
    "\n",
    "This embedding will take the data from being 784 dimensional down to only 2 dimensions, so that we can plot the data. In the plot, each color represents a different digit classification.\n",
    "\n",
    "Lets try running the UMAP algorithm with the parameters\n",
    " - ```n_components``` = 2\n",
    " - ```n_neighbors``` = 20\n",
    " - ```min_dist``` = 0.001\n",
    " \n",
    " (these are the three most important parameters in the UMAP algorithm! Remember them!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_embedding = umap.UMAP(\n",
    "    n_neighbors=20,\n",
    "    n_components=2,\n",
    "    min_dist=0.001 # NB I have not discussed this parameter yet!\n",
    ").fit_transform(mnist.data) # EXERCISE: FILL IN HERE\n",
    "\n",
    "print(f'Shape of umap_embedding data: {umap_embedding.shape}')\n",
    "\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "plt.scatter(umap_embedding[:, 0], umap_embedding[:, 1], c=mnist.target.astype(int), s=0.5, cmap='Spectral')\n",
    "cbar = plt.colorbar(boundaries=np.arange(11)-0.5)\n",
    "cbar.set_ticks(np.arange(10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering using $k$-means directly on the high dimensional data\n",
    "\n",
    "Here we use $k$-means from scikit-learn (which we imported earlier) directly on the high dimensional data, and find that... it fails! We see that the labels are all mixed up, totally unlike the original labels.\n",
    "\n",
    "In doing this sequence of operations, we are using UMAP as a _vizualisation_ tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_labels = KMeans(n_clusters=10).fit_predict(mnist.data)\n",
    "\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "plt.scatter(umap_embedding[:, 0], umap_embedding[:, 1], c=kmeans_labels, s=0.5, cmap='Spectral');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the adjusted random score and adjusted mutual information score. These are standard indicators of the quality of clustering results, available in ```scikit-learn```.\n",
    "\n",
    "For both, a result close to 1 is good, a result close to 0 is bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score\n",
    "\n",
    "print(f'Adjusted rand score: {adjusted_rand_score(mnist.target, kmeans_labels)}')\n",
    "print(f'Adjusted mutual info: {adjusted_mutual_info_score(mnist.target, kmeans_labels)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering using $k$-means on the UMAP embedded data\n",
    "\n",
    "Now we're going to try using $k$-means clustering on the _lower dimensional data_, and hopefully things will work a bit better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_labels = KMeans(n_clusters=10).fit_predict(umap_embedding)\n",
    "\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "plt.scatter(umap_embedding[:, 0], umap_embedding[:, 1], c=kmeans_labels, s=0.5, cmap='Spectral');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Adjusted rand score: {adjusted_rand_score(mnist.target, kmeans_labels)}')\n",
    "print(f'Adjusted mutual info: {adjusted_mutual_info_score(mnist.target, kmeans_labels)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try a more advanced clustering technique: HBDSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_labels = hdbscan.HDBSCAN(min_samples=10, min_cluster_size=500).fit_predict(umap_embedding)\n",
    "\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "clustered = (hdbscan_labels >= 0)\n",
    "plt.scatter(umap_embedding[~clustered, 0],\n",
    "            umap_embedding[~clustered, 1],\n",
    "            c=(0.5, 0.5, 0.5),\n",
    "            s=0.1,\n",
    "            alpha=0.5)\n",
    "plt.scatter(umap_embedding[clustered, 0],\n",
    "            umap_embedding[clustered, 1],\n",
    "            c=hdbscan_labels[clustered],\n",
    "            s=0.1,\n",
    "            cmap='Spectral');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Adjusted rand score: {adjusted_rand_score(mnist.target, hdbscan_labels)}')\n",
    "print(f'Adjusted mutual info: {adjusted_mutual_info_score(mnist.target, hdbscan_labels)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion: what we have learned today\n",
    "\n",
    "1. ### High dimensional data points are difficult to distinguish for mathematical reasons\n",
    "2. ### Clustering (of just about any flavour) fails in high dimensional settings for these mathematical reasons\n",
    "3. ### Data often approximately lives on a low dimensional manifold in ambient space\n",
    "4. ### UMAP is a tool to _detect_ that manifold _and_ embed the data from the ambient space to a _lower dimensional space_.\n",
    "5. ### Clustering on the lower dimensional UMAP-embedded data can perform extremely well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issues and limitations of UMAP\n",
    " \n",
    " - Unlike classical methods of regression or PCA analysis, UMAP has relatively few tools to determine statistical success. How do we determine goodness-of-fit of a UMAP embedding? There is not much literature on this.\n",
    " \n",
    " - The embedding created by UMAP is a \"black box\". There is no simple analytical understanding of what UMAP has actually done to the data. Although UMAP _can_ provide an inverse transform, there is no analytical way to use this inverse transform, e.g. for density estimation in the original high dimensional space.\n",
    " \n",
    " - The underlying mathematical assumption of UMAP is that **the data is uniformly distributed on the underlying manifold**. This assumption isn't necessarily true of most data sets. Perhaps it is a flawed point-of-view?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
